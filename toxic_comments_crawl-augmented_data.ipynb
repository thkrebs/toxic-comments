{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model for the Toxic Comment Competition on Kaggle using FastText\n",
    "\n",
    "https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge\n",
    "\n",
    "Based on the kernel from Jeremy Howard published on kaggle: https://www.kaggle.com/jhoward/improved-lstm-baseline-glove-dropout\n",
    "\n",
    "Instead of using the Glove embedding, this is using the fasttext embedding (crawl_30d-2M.vec). I preprocessed the embedding with embedding_utils.py in order to check for dimensions and remove the first line\n",
    "\n",
    "Further, it leverages the idea of Pavel Ostyakovâ€™s discussed in the kaggle forum to augment the data sets by using machine translation of the provided data. Basically, I augmented just the training set with data resulting from translating the comments fromm its original into the languages DE, ES and FR and then back to english.\n",
    "This additional data was added to the training set leading to about 650k labeled comments. I split of the validation set from the original data.\n",
    "\n",
    "With that approach the validiation Accuracy improved to: 98.970% which is an improvement of about 0.5% compared to the version just using the Fasttext embedding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, re, csv, codecs, logging, numpy as np, pandas as pd\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, BatchNormalization\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, Callback\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/'\n",
    "comp = 'jigsaw-toxic-comment-classification-challenge/'\n",
    "EMBEDDING_FILE=f'{path}crawl_300d-2M_processed.vec'\n",
    "#EMBEDDING_FILE=f'{path}glove.6B.50d.txt'\n",
    "TRAIN_DATA_FILE=f'{path}{comp}train.csv'\n",
    "TEST_DATA_FILE=f'{path}{comp}test.csv'\n",
    "TRAIN_DATA_AUGMENTED=[f'{path}train_de.csv', f'{path}train_fr.csv', f'{path}train_es.csv']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set some basic config parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size=300\n",
    "max_features = 60000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 400 # max number of words in a comment to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in our data and replace missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillna(dataframe):\n",
    "    df = dataframe\n",
    "    return df[\"comment_text\"].fillna(\"_na_\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(TRAIN_DATA_FILE)\n",
    "test = pd.read_csv(TEST_DATA_FILE)\n",
    "\n",
    "list_sentences_train = fillna(train)\n",
    "list_sentences_test = fillna(test)\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "\n",
    "\n",
    "train_y = train[list_classes].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Augment training dats with datasets which are derived by translating them from english to an intermediate language (german, french, spanish) and back to english. This approach augments the training set significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(f'{path}train_de.csv')\n",
    "df2 = pd.read_csv(f'{path}train_es.csv')\n",
    "df3 = pd.read_csv(f'{path}train_fr.csv')\n",
    "\n",
    "a_y1 = df1[list_classes].values\n",
    "a_y2 = df2[list_classes].values\n",
    "a_y3 = df3[list_classes].values\n",
    "\n",
    "list_a_y = np.append(a_y1, values = a_y2, axis=0)\n",
    "list_a_y = np.append(list_a_y, values = a_y2, axis=0)\n",
    "\n",
    "list_a_train = np.append(fillna(df1),values=fillna(df2), axis=0)\n",
    "list_a_train = np.append(list_a_train,values=fillna(df3), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(478713,)\n"
     ]
    }
   ],
   "source": [
    "print(list_a_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard keras preprocessing, to turn each comment into a list of word indexes of equal length (with truncation or padding as needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "\n",
    "# we are fitting the tokenizer on the full set of data including the augmented data\n",
    "all_train = np.append(list_sentences_train,list_a_train)\n",
    "tokenizer.fit_on_texts(all_train)\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "list_tokenized_train_a = tokenizer.texts_to_sequences(list_a_train)                     \n",
    "X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_t_a = pad_sequences(list_tokenized_train_a, maxlen=maxlen)\n",
    "X_te = pad_sequences(list_tokenized_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_y = np.append(train_y, list_a_y )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the embeddings vectors (space delimited strings) into a dictionary from word->vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_embedding_list(file_path):\n",
    "    embedding_word_dict = {}\n",
    "    embedding_list = []\n",
    "    i = 0\n",
    "    with open(file_path) as f:\n",
    "        for row in f:\n",
    "            data = row.split(\" \")\n",
    "            word = data[0]\n",
    "            embedding = np.array([float(num) for num in data[1:-1]])\n",
    "            embedding_list.append(embedding)\n",
    "            embedding_word_dict[word] = embedding\n",
    " \n",
    "    embedding_list = np.array(embedding_list)\n",
    "    return embedding_list, embedding_word_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use these vectors to create our embedding matrix, with random initialization for words that aren't in Fasttext. We'll use the same mean and stdev of embeddings the Fasttext has when generating the random init."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_list, embeddings_index = read_embedding_list(EMBEDDING_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine mean values and standard deviation to initialize the embedding matrix. This ensures that all words which are not found in the embeddings are set to random value around the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.005528633668833318, 0.34703942181861597)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "emb_mean,emb_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple bidirectional LSTM with two fully connected layers. We add some dropout to the LSTM since even 2 epochs is enough to overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch+1, score))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(maxlen,))\n",
    "x = Embedding(max_features, embed_size, weights=[embedding_matrix],trainable = False)(inp)\n",
    "x = Bidirectional(LSTM(128, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(50, activation=\"relu\")(x)\n",
    "x = Dropout(0.4)(x)\n",
    "x = Dense(6, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to fit out model! Use `validation_split` when not submitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/ml/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_tra   : (143613, 400)\n",
      "Shape of X_t_a   : (478713, 400)\n",
      "Shape of X_tra   : (622326, 400)\n",
      "Shape of y_tra   : (622326, 6)\n"
     ]
    }
   ],
   "source": [
    "X_tra = []\n",
    "y_tra = []\n",
    "\n",
    "# we split on the original test data and then add the augmented data to the training set\n",
    "# that ensures that we do not have the original data and augmented data on the same side\n",
    "X_tra, X_val, y_tra, y_val = train_test_split(X_t, train_y, train_size=0.90, random_state=275)\n",
    "print(\"Shape of X_tra   : \" + str(X_tra.shape))\n",
    "print(\"Shape of X_t_a   : \" + str(X_t_a.shape))\n",
    "X_tra = np.append(X_tra, X_t_a, axis=0)\n",
    "\n",
    "print(\"Shape of X_tra   : \" + str(X_tra.shape))\n",
    "y_tra = np.append(y_tra, list_a_y, axis=0)\n",
    "print(\"Shape of y_tra   : \" + str(y_tra.shape))\n",
    "RocAuc = RocAucEvaluation(validation_data=(X_val, y_val), interval=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 622326 samples, validate on 15958 samples\n",
      "Epoch 1/20\n",
      "622326/622326 [==============================] - 9401s 15ms/step - loss: 0.0518 - acc: 0.9811 - val_loss: 0.0410 - val_acc: 0.9841\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.98405, saving model to crawl_augmented_weights.best.model.hdf5\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.988710 \n",
      "\n",
      "Epoch 2/20\n",
      "622326/622326 [==============================] - 9404s 15ms/step - loss: 0.0464 - acc: 0.9825 - val_loss: 0.0364 - val_acc: 0.9853\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.98405 to 0.98527, saving model to crawl_augmented_weights.best.model.hdf5\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.991358 \n",
      "\n",
      "Epoch 3/20\n",
      "622326/622326 [==============================] - 9709s 16ms/step - loss: 0.0434 - acc: 0.9833 - val_loss: 0.0346 - val_acc: 0.9857\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.98527 to 0.98572, saving model to crawl_augmented_weights.best.model.hdf5\n",
      "\n",
      " ROC-AUC - epoch: 3 - score: 0.992253 \n",
      "\n",
      "Epoch 4/20\n",
      "622326/622326 [==============================] - 9744s 16ms/step - loss: 0.0409 - acc: 0.9841 - val_loss: 0.0328 - val_acc: 0.9863\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.98572 to 0.98633, saving model to crawl_augmented_weights.best.model.hdf5\n",
      "\n",
      " ROC-AUC - epoch: 4 - score: 0.993277 \n",
      "\n",
      "Epoch 5/20\n",
      "622326/622326 [==============================] - 9514s 15ms/step - loss: 0.0390 - acc: 0.9847 - val_loss: 0.0322 - val_acc: 0.9871\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.98633 to 0.98710, saving model to crawl_augmented_weights.best.model.hdf5\n",
      "\n",
      " ROC-AUC - epoch: 5 - score: 0.993645 \n",
      "\n",
      "Epoch 6/20\n",
      "622326/622326 [==============================] - 29658s 48ms/step - loss: 0.0375 - acc: 0.9852 - val_loss: 0.0298 - val_acc: 0.9880\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.98710 to 0.98797, saving model to crawl_augmented_weights.best.model.hdf5\n",
      "\n",
      " ROC-AUC - epoch: 6 - score: 0.994248 \n",
      "\n",
      "Epoch 7/20\n",
      "622326/622326 [==============================] - 9719s 16ms/step - loss: 0.0360 - acc: 0.9857 - val_loss: 0.0285 - val_acc: 0.9884\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.98797 to 0.98841, saving model to crawl_augmented_weights.best.model.hdf5\n",
      "\n",
      " ROC-AUC - epoch: 7 - score: 0.995008 \n",
      "\n",
      "Epoch 8/20\n",
      "622326/622326 [==============================] - 9631s 15ms/step - loss: 0.0345 - acc: 0.9862 - val_loss: 0.0286 - val_acc: 0.9880\n",
      "\n",
      "Epoch 00008: val_acc did not improve\n",
      "\n",
      " ROC-AUC - epoch: 8 - score: 0.995303 \n",
      "\n",
      "Epoch 9/20\n",
      "622326/622326 [==============================] - 9831s 16ms/step - loss: 0.0337 - acc: 0.9865 - val_loss: 0.0279 - val_acc: 0.9886\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.98841 to 0.98857, saving model to crawl_augmented_weights.best.model.hdf5\n",
      "\n",
      " ROC-AUC - epoch: 9 - score: 0.995517 \n",
      "\n",
      "Epoch 10/20\n",
      "622326/622326 [==============================] - 9739s 16ms/step - loss: 0.0329 - acc: 0.9869 - val_loss: 0.0273 - val_acc: 0.9886\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.98857 to 0.98862, saving model to crawl_augmented_weights.best.model.hdf5\n",
      "\n",
      " ROC-AUC - epoch: 10 - score: 0.995195 \n",
      "\n",
      "Epoch 11/20\n",
      "622326/622326 [==============================] - 9539s 15ms/step - loss: 0.0321 - acc: 0.9871 - val_loss: 0.0266 - val_acc: 0.9891\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.98862 to 0.98915, saving model to crawl_augmented_weights.best.model.hdf5\n",
      "\n",
      " ROC-AUC - epoch: 11 - score: 0.995675 \n",
      "\n",
      "Epoch 12/20\n",
      "622326/622326 [==============================] - 9504s 15ms/step - loss: 0.0311 - acc: 0.9875 - val_loss: 0.0255 - val_acc: 0.9896\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.98915 to 0.98957, saving model to crawl_augmented_weights.best.model.hdf5\n",
      "\n",
      " ROC-AUC - epoch: 12 - score: 0.996150 \n",
      "\n",
      "Epoch 13/20\n",
      "622326/622326 [==============================] - 36984s 59ms/step - loss: 0.0304 - acc: 0.9877 - val_loss: 0.0250 - val_acc: 0.9897\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.98957 to 0.98970, saving model to crawl_augmented_weights.best.model.hdf5\n",
      "\n",
      " ROC-AUC - epoch: 13 - score: 0.996194 \n",
      "\n",
      "Epoch 14/20\n",
      "622326/622326 [==============================] - 9736s 16ms/step - loss: 0.0303 - acc: 0.9878 - val_loss: 0.0254 - val_acc: 0.9893\n",
      "\n",
      "Epoch 00014: val_acc did not improve\n",
      "\n",
      " ROC-AUC - epoch: 14 - score: 0.996095 \n",
      "\n",
      "Epoch 15/20\n",
      "622326/622326 [==============================] - 26866s 43ms/step - loss: 0.0298 - acc: 0.9880 - val_loss: 0.0247 - val_acc: 0.9895\n",
      "\n",
      "Epoch 00015: val_acc did not improve\n",
      "\n",
      " ROC-AUC - epoch: 15 - score: 0.996275 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "filepath=\"crawl_augmented_weights.best.model\" + \".hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "early_stop = EarlyStopping(monitor='val_acc', patience=2, mode='max') \n",
    "callbacks_list = [checkpoint, early_stop, RocAuc]\n",
    "\n",
    "hist = model.fit(X_tra, y_tra, batch_size=64, epochs=20, validation_data=(X_val, y_val),callbacks=callbacks_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153164/153164 [==============================] - 983s 6ms/step\n"
     ]
    }
   ],
   "source": [
    "y_test = model.predict([X_te], batch_size=2048, verbose=1)\n",
    "sample_submission = pd.read_csv(f'{path}{comp}sample_submission.csv')\n",
    "sample_submission[list_classes] = y_test\n",
    "sample_submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking prediction with a sample from test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yo bitch Ja Rule is more succesful then you'll ever be whats up with you and hating you sad mofuckas...i should bitch slap ur pethedic white faces and get you to kiss my ass you guys sicken me. Ja rule is about pride in da music man. dont diss that shit on him. and nothin is wrong bein like tupac he was a brother too...fuckin white boys get things right next time.,\n",
      "----------------------------------\n",
      "== From RfC == \n",
      "\n",
      " The title is fine as it is, IMO.\n"
     ]
    }
   ],
   "source": [
    "test_sample = list_sentences_test[0:2].reshape(2,)\n",
    "print(test_sample[0])\n",
    "print(\"----------------------------------\")\n",
    "print(test_sample[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sample = tokenizer.texts_to_sequences(test_sample)\n",
    "X_sample = pad_sequences(tokenized_sample, maxlen=maxlen)\n",
    "\n",
    "y_test = model.predict([X_sample],verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.9991131e-01 3.5480884e-01 9.9813110e-01 1.5038978e-03 9.7947401e-01\n",
      "  6.3801211e-01]\n",
      " [4.7656525e-07 1.8563962e-15 5.0761123e-08 2.9353562e-21 9.4404473e-10\n",
      "  2.9448616e-17]]\n"
     ]
    }
   ],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Row1</th>\n",
       "      <td>Yo bitch Ja Rule is more succesful then you'll...</td>\n",
       "      <td>0.999911</td>\n",
       "      <td>0.354809</td>\n",
       "      <td>0.998131</td>\n",
       "      <td>0.0015039</td>\n",
       "      <td>0.979474</td>\n",
       "      <td>0.638012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Row2</th>\n",
       "      <td>== From RfC == \\n\\n The title is fine as it is...</td>\n",
       "      <td>4.76565e-07</td>\n",
       "      <td>1.8564e-15</td>\n",
       "      <td>5.07611e-08</td>\n",
       "      <td>2.93536e-21</td>\n",
       "      <td>9.44045e-10</td>\n",
       "      <td>2.94486e-17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                comment        toxic  \\\n",
       "Row1  Yo bitch Ja Rule is more succesful then you'll...     0.999911   \n",
       "Row2  == From RfC == \\n\\n The title is fine as it is...  4.76565e-07   \n",
       "\n",
       "     severe_toxic      obscene       threat       insult identity_hate  \n",
       "Row1     0.354809     0.998131    0.0015039     0.979474      0.638012  \n",
       "Row2   1.8564e-15  5.07611e-08  2.93536e-21  9.44045e-10   2.94486e-17  "
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heading = np.append([\"comment\"], [list_classes]).reshape(1,7)\n",
    "dd = np.append(test_sample.reshape(2,1), y_test, axis=1)\n",
    "\n",
    "index = ['Row'+str(i) for i in range(1, len(dd)+1)]\n",
    "df = pd.DataFrame(dd, index, columns=heading[0,0:])\n",
    "df.tail()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Row1 has prob. of being toxic, insulting and obscene\n",
    "\n",
    "Row2 can be considered as being a clean comment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
