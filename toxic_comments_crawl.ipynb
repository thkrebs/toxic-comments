{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model for the Toxic Comment Competition on Kaggle using FastText\n",
    "\n",
    "https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge\n",
    "\n",
    "Based on the kernel from Jeremy Howard published on kaggle: https://www.kaggle.com/jhoward/improved-lstm-baseline-glove-dropout\n",
    "\n",
    "Instead of using the Glove embedding, this is using the fasttext embedding (crawl_30d-2M.vec). I preprocessed the embedding with embedding_utils.py in order to check for dimensions and remove the first line\n",
    "\n",
    "Validiation Accuracy: 98,425% which is an improvement of about 0.2% compared to baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/ml/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys, os, re, csv, codecs, logging, numpy as np, pandas as pd\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, BatchNormalization\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, Callback\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We include the GloVe word vectors in our input files. To include these in your kernel, simple click 'input files' at the top of the notebook, and search 'glove' in the 'datasets' section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/'\n",
    "comp = 'jigsaw-toxic-comment-classification-challenge/'\n",
    "EMBEDDING_FILE=f'{path}crawl_300d-2M_processed.vec'\n",
    "#EMBEDDING_FILE=f'{path}glove.6B.50d.txt'\n",
    "TRAIN_DATA_FILE=f'{path}{comp}train.csv'\n",
    "TEST_DATA_FILE=f'{path}{comp}test.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set some basic config parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embed_size = 50 # how big is each word vector\n",
    "embed_size=300\n",
    "max_features = 60000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 400 # max number of words in a comment to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in our data and replace missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(TRAIN_DATA_FILE)\n",
    "test = pd.read_csv(TEST_DATA_FILE)\n",
    "\n",
    "list_sentences_train = train[\"comment_text\"].fillna(\"_na_\").values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"_na_\").values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard keras preprocessing, to turn each comment into a list of word indexes of equal length (with truncation or padding as needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_te = pad_sequences(list_tokenized_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the embeddings vectors (space delimited strings) into a dictionary from word->vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_embedding_list(file_path):\n",
    "    embedding_word_dict = {}\n",
    "    embedding_list = []\n",
    "    i = 0\n",
    "    with open(file_path) as f:\n",
    "        for row in f:\n",
    "            data = row.split(\" \")\n",
    "            word = data[0]\n",
    "            embedding = np.array([float(num) for num in data[1:-1]])\n",
    "            embedding_list.append(embedding)\n",
    "            embedding_word_dict[word] = embedding\n",
    " \n",
    "    embedding_list = np.array(embedding_list)\n",
    "    return embedding_list, embedding_word_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use these vectors to create our embedding matrix, with random initialization for words that aren't in Fasttext. We'll use the same mean and stdev of embeddings the Fasttext has when generating the random init."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_list, embeddings_index = read_embedding_list(EMBEDDING_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine mean values and standard deviation to initialize the embedding matrix. This ensures that all words which are not found in the embeddings are set to random value around the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.005528633668833318, 0.34703942181861597)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "emb_mean,emb_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple bidirectional LSTM with two fully connected layers. We add some dropout to the LSTM since even 2 epochs is enough to overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(maxlen,))\n",
    "x = Embedding(max_features, embed_size, weights=[embedding_matrix],trainable = False)(inp)\n",
    "x = Bidirectional(LSTM(128, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(50, activation=\"relu\")(x)\n",
    "x = Dropout(0.4)(x)\n",
    "x = Dense(6, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to fit out model! Use `validation_split` when not submitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 400)\n",
      "(159571, 6)\n"
     ]
    }
   ],
   "source": [
    "print(X_t.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/20\n",
      "143613/143613 [==============================] - 2274s 16ms/step - loss: 0.0655 - acc: 0.9776 - val_loss: 0.0481 - val_acc: 0.9819\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.98187, saving model to crawl_weights.best.model.hdf5\n",
      "Epoch 2/20\n",
      "143613/143613 [==============================] - 2337s 16ms/step - loss: 0.0488 - acc: 0.9818 - val_loss: 0.0446 - val_acc: 0.9830\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.98187 to 0.98297, saving model to crawl_weights.best.model.hdf5\n",
      "Epoch 3/20\n",
      "143613/143613 [==============================] - 2248s 16ms/step - loss: 0.0456 - acc: 0.9825 - val_loss: 0.0432 - val_acc: 0.9836\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.98297 to 0.98363, saving model to crawl_weights.best.model.hdf5\n",
      "Epoch 4/20\n",
      "143613/143613 [==============================] - 2260s 16ms/step - loss: 0.0436 - acc: 0.9833 - val_loss: 0.0440 - val_acc: 0.9833\n",
      "\n",
      "Epoch 00004: val_acc did not improve\n",
      "Epoch 5/20\n",
      "143613/143613 [==============================] - 2257s 16ms/step - loss: 0.0421 - acc: 0.9835 - val_loss: 0.0422 - val_acc: 0.9839\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.98363 to 0.98385, saving model to crawl_weights.best.model.hdf5\n",
      "Epoch 6/20\n",
      "143613/143613 [==============================] - 2248s 16ms/step - loss: 0.0407 - acc: 0.9839 - val_loss: 0.0411 - val_acc: 0.9840\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.98385 to 0.98397, saving model to crawl_weights.best.model.hdf5\n",
      "Epoch 7/20\n",
      "143613/143613 [==============================] - 2254s 16ms/step - loss: 0.0394 - acc: 0.9843 - val_loss: 0.0418 - val_acc: 0.9840\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.98397 to 0.98400, saving model to crawl_weights.best.model.hdf5\n",
      "Epoch 8/20\n",
      "143613/143613 [==============================] - 4077s 28ms/step - loss: 0.0382 - acc: 0.9847 - val_loss: 0.0414 - val_acc: 0.9839\n",
      "\n",
      "Epoch 00008: val_acc did not improve\n",
      "Epoch 9/20\n",
      "143613/143613 [==============================] - 2268s 16ms/step - loss: 0.0371 - acc: 0.9850 - val_loss: 0.0428 - val_acc: 0.9843\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.98400 to 0.98425, saving model to crawl_weights.best.model.hdf5\n",
      "Epoch 10/20\n",
      "143613/143613 [==============================] - 2287s 16ms/step - loss: 0.0361 - acc: 0.9854 - val_loss: 0.0421 - val_acc: 0.9842\n",
      "\n",
      "Epoch 00010: val_acc did not improve\n",
      "Epoch 11/20\n",
      "143613/143613 [==============================] - 2275s 16ms/step - loss: 0.0351 - acc: 0.9858 - val_loss: 0.0423 - val_acc: 0.9839\n",
      "\n",
      "Epoch 00011: val_acc did not improve\n"
     ]
    }
   ],
   "source": [
    "filepath=\"crawl_weights.best.model\" + \".hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "early_stop = EarlyStopping(monitor='val_acc', patience=2, mode='max') \n",
    "callbacks_list = [checkpoint, early_stop]\n",
    "\n",
    "model.fit(X_t, y, batch_size=64, epochs=20, validation_split=0.1,callbacks=callbacks_list);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153164/153164 [==============================] - 933s 6ms/step\n"
     ]
    }
   ],
   "source": [
    "y_test = model.predict([X_te], batch_size=1024, verbose=1)\n",
    "sample_submission = pd.read_csv(f'{path}{comp}sample_submission.csv')\n",
    "sample_submission[list_classes] = y_test\n",
    "sample_submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking prediction with a sample from test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yo bitch Ja Rule is more succesful then you'll ever be whats up with you and hating you sad mofuckas...i should bitch slap ur pethedic white faces and get you to kiss my ass you guys sicken me. Ja rule is about pride in da music man. dont diss that shit on him. and nothin is wrong bein like tupac he was a brother too...fuckin white boys get things right next time.,\n",
      "----------------------------------\n",
      "== From RfC == \n",
      "\n",
      " The title is fine as it is, IMO.\n"
     ]
    }
   ],
   "source": [
    "test_sample = list_sentences_test[0:2].reshape(2,)\n",
    "print(test_sample[0])\n",
    "print(\"----------------------------------\")\n",
    "print(test_sample[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sample = tokenizer.texts_to_sequences(test_sample)\n",
    "X_sample = pad_sequences(tokenized_sample, maxlen=maxlen)\n",
    "\n",
    "y_test = model.predict([X_sample],verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.9999166e-01 3.9733967e-01 9.8811215e-01 3.5311554e-02 9.4863749e-01\n",
      "  3.9724001e-01]\n",
      " [1.5717048e-04 3.3678338e-11 1.5220577e-06 4.5525203e-10 2.4704607e-07\n",
      "  1.8052205e-08]]\n"
     ]
    }
   ],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Row1</th>\n",
       "      <td>Yo bitch Ja Rule is more succesful then you'll...</td>\n",
       "      <td>0.999992</td>\n",
       "      <td>0.39734</td>\n",
       "      <td>0.988112</td>\n",
       "      <td>0.0353116</td>\n",
       "      <td>0.948637</td>\n",
       "      <td>0.39724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Row2</th>\n",
       "      <td>== From RfC == \\n\\n The title is fine as it is...</td>\n",
       "      <td>0.00015717</td>\n",
       "      <td>3.36783e-11</td>\n",
       "      <td>1.52206e-06</td>\n",
       "      <td>4.55252e-10</td>\n",
       "      <td>2.47046e-07</td>\n",
       "      <td>1.80522e-08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                comment       toxic  \\\n",
       "Row1  Yo bitch Ja Rule is more succesful then you'll...    0.999992   \n",
       "Row2  == From RfC == \\n\\n The title is fine as it is...  0.00015717   \n",
       "\n",
       "     severe_toxic      obscene       threat       insult identity_hate  \n",
       "Row1      0.39734     0.988112    0.0353116     0.948637       0.39724  \n",
       "Row2  3.36783e-11  1.52206e-06  4.55252e-10  2.47046e-07   1.80522e-08  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heading = np.append([\"comment\"], [list_classes]).reshape(1,7)\n",
    "dd = np.append(test_sample.reshape(2,1), y_test, axis=1)\n",
    "\n",
    "index = ['Row'+str(i) for i in range(1, len(dd)+1)]\n",
    "df = pd.DataFrame(dd, index, columns=heading[0,0:])\n",
    "df.tail()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Row1 has prob. of being toxic, insulting and obscene\n",
    "\n",
    "Row2 can be considered as being a clean comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
