{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model for the Toxic Comment Competition on Kaggle\n",
    "\n",
    "https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge\n",
    "\n",
    "Based on the kernel from Jeremy Howard published on kaggle: https://www.kaggle.com/jhoward/improved-lstm-baseline-glove-dropout\n",
    "\n",
    "Accuracy on validation set: 98.22%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys, os, re, csv, codecs, logging, numpy as np, pandas as pd\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, BatchNormalization\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, Callback\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We include the GloVe word vectors in our input files. To include these in your kernel, simple click 'input files' at the top of the notebook, and search 'glove' in the 'datasets' section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/'\n",
    "comp = 'jigsaw-toxic-comment-classification-challenge/'\n",
    "EMBEDDING_FILE=f'{path}glove.6B.50d.txt'\n",
    "TRAIN_DATA_FILE=f'{path}{comp}train.csv'\n",
    "TEST_DATA_FILE=f'{path}{comp}test.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set some basic config parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 50 # how big is each word vector\n",
    "max_features = 60000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 400 # max number of words in a comment to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in our data and replace missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(TRAIN_DATA_FILE)\n",
    "test = pd.read_csv(TEST_DATA_FILE)\n",
    "\n",
    "list_sentences_train = train[\"comment_text\"].fillna(\"_na_\").values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"_na_\").values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard keras preprocessing, to turn each comment into a list of word indexes of equal length (with truncation or padding as needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_te = pad_sequences(list_tokenized_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the glove word vectors (space delimited strings) into a dictionary from word->vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.strip().split()) for o in open(EMBEDDING_FILE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use these vectors to create our embedding matrix, with random initialization for words that aren't in GloVe. We'll use the same mean and stdev of embeddings the GloVe has when generating the random init."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.020940498, 0.6441043)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "emb_mean,emb_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple bidirectional LSTM with two fully connected layers. We add some dropout to the LSTM since even 2 epochs is enough to overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(maxlen,))\n",
    "x = Embedding(max_features, embed_size, weights=[embedding_matrix],trainable = False)(inp)\n",
    "x = Bidirectional(LSTM(128, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(50, activation=\"relu\")(x)\n",
    "x = Dropout(0.4)(x)\n",
    "x = Dense(6, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to fit out model! Use `validation_split` when not submitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/20\n",
      "143613/143613 [==============================] - 3032s 21ms/step - loss: 0.0790 - acc: 0.9740 - val_loss: 0.0597 - val_acc: 0.9787\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.97871, saving model to weights.best.model.hdf5\n",
      "Epoch 2/20\n",
      "143613/143613 [==============================] - 2908s 20ms/step - loss: 0.0609 - acc: 0.9789 - val_loss: 0.0555 - val_acc: 0.9800\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.97871 to 0.98002, saving model to weights.best.model.hdf5\n",
      "Epoch 3/20\n",
      "143613/143613 [==============================] - 2721s 19ms/step - loss: 0.0566 - acc: 0.9799 - val_loss: 0.0522 - val_acc: 0.9807\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.98002 to 0.98067, saving model to weights.best.model.hdf5\n",
      "Epoch 4/20\n",
      "143613/143613 [==============================] - 2723s 19ms/step - loss: 0.0543 - acc: 0.9804 - val_loss: 0.0518 - val_acc: 0.9815\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.98067 to 0.98149, saving model to weights.best.model.hdf5\n",
      "Epoch 5/20\n",
      "143613/143613 [==============================] - 3024s 21ms/step - loss: 0.0523 - acc: 0.9809 - val_loss: 0.0495 - val_acc: 0.9815\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.98149 to 0.98151, saving model to weights.best.model.hdf5\n",
      "Epoch 6/20\n",
      "143613/143613 [==============================] - 2893s 20ms/step - loss: 0.0505 - acc: 0.9815 - val_loss: 0.0486 - val_acc: 0.9817\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.98151 to 0.98166, saving model to weights.best.model.hdf5\n",
      "Epoch 7/20\n",
      "143613/143613 [==============================] - 2566s 18ms/step - loss: 0.0495 - acc: 0.9817 - val_loss: 0.0484 - val_acc: 0.9821\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.98166 to 0.98210, saving model to weights.best.model.hdf5\n",
      "Epoch 8/20\n",
      "143613/143613 [==============================] - 2564s 18ms/step - loss: 0.0481 - acc: 0.9821 - val_loss: 0.0470 - val_acc: 0.9823\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.98210 to 0.98233, saving model to weights.best.model.hdf5\n",
      "Epoch 9/20\n",
      "143613/143613 [==============================] - 2570s 18ms/step - loss: 0.0469 - acc: 0.9823 - val_loss: 0.0472 - val_acc: 0.9825\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.98233 to 0.98245, saving model to weights.best.model.hdf5\n",
      "Epoch 10/20\n",
      "143613/143613 [==============================] - 2565s 18ms/step - loss: 0.0462 - acc: 0.9825 - val_loss: 0.0485 - val_acc: 0.9822\n",
      "\n",
      "Epoch 00010: val_acc did not improve\n",
      "Epoch 11/20\n",
      "143613/143613 [==============================] - 26768s 186ms/step - loss: 0.0454 - acc: 0.9828 - val_loss: 0.0468 - val_acc: 0.9824\n",
      "\n",
      "Epoch 00011: val_acc did not improve\n"
     ]
    }
   ],
   "source": [
    "filepath=\"weights.best.model\" + \".hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "early_stop = EarlyStopping(monitor='val_acc', patience=2, mode='max') \n",
    "callbacks_list = [checkpoint, early_stop]\n",
    "\n",
    "model.fit(X_t, y, batch_size=64, epochs=20, validation_split=0.1,callbacks=callbacks_list);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153164/153164 [==============================] - 565s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "y_test = model.predict([X_te], batch_size=1024, verbose=1)\n",
    "sample_submission = pd.read_csv(f'{path}{comp}sample_submission.csv')\n",
    "sample_submission[list_classes] = y_test\n",
    "sample_submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking prediction with a sample from test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"weights.best.model.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yo bitch Ja Rule is more succesful then you'll ever be whats up with you and hating you sad mofuckas...i should bitch slap ur pethedic white faces and get you to kiss my ass you guys sicken me. Ja rule is about pride in da music man. dont diss that shit on him. and nothin is wrong bein like tupac he was a brother too...fuckin white boys get things right next time.,\n",
      "----------------------------------\n",
      "== From RfC == \n",
      "\n",
      " The title is fine as it is, IMO.\n",
      "----------------------------------\n",
      "\" \n",
      "\n",
      " == Sources == \n",
      "\n",
      " * Zawe Ashton on Lapland —  /  \"\n"
     ]
    }
   ],
   "source": [
    "test_sample = list_sentences_test[0:3].reshape(3,)\n",
    "print(test_sample[0])\n",
    "print(\"----------------------------------\")\n",
    "print(test_sample[1])\n",
    "print(\"----------------------------------\")\n",
    "print(test_sample[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sample = tokenizer.texts_to_sequences(test_sample)\n",
    "X_sample = pad_sequences(tokenized_sample, maxlen=maxlen)\n",
    "\n",
    "y_test = model.predict([X_sample],verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Row1</th>\n",
       "      <td>Yo bitch Ja Rule is more succesful then you'll...</td>\n",
       "      <td>0.999616</td>\n",
       "      <td>0.400736</td>\n",
       "      <td>0.964734</td>\n",
       "      <td>0.10676</td>\n",
       "      <td>0.886818</td>\n",
       "      <td>0.394428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Row2</th>\n",
       "      <td>== From RfC == \\n\\n The title is fine as it is...</td>\n",
       "      <td>0.000626001</td>\n",
       "      <td>1.39597e-08</td>\n",
       "      <td>0.00015868</td>\n",
       "      <td>1.95521e-07</td>\n",
       "      <td>3.66872e-05</td>\n",
       "      <td>9.68649e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Row3</th>\n",
       "      <td>\" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...</td>\n",
       "      <td>0.000235214</td>\n",
       "      <td>3.05182e-09</td>\n",
       "      <td>3.70316e-05</td>\n",
       "      <td>1.34545e-07</td>\n",
       "      <td>9.7508e-06</td>\n",
       "      <td>2.96946e-06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                comment        toxic  \\\n",
       "Row1  Yo bitch Ja Rule is more succesful then you'll...     0.999616   \n",
       "Row2  == From RfC == \\n\\n The title is fine as it is...  0.000626001   \n",
       "Row3  \" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...  0.000235214   \n",
       "\n",
       "     severe_toxic      obscene       threat       insult identity_hate  \n",
       "Row1     0.400736     0.964734      0.10676     0.886818      0.394428  \n",
       "Row2  1.39597e-08   0.00015868  1.95521e-07  3.66872e-05   9.68649e-06  \n",
       "Row3  3.05182e-09  3.70316e-05  1.34545e-07   9.7508e-06   2.96946e-06  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heading = np.append([\"comment\"], [list_classes]).reshape(1,7)\n",
    "dd = np.append(test_sample.reshape(3,1), y_test, axis=1)\n",
    "\n",
    "index = ['Row'+str(i) for i in range(1, len(dd)+1)]\n",
    "df = pd.DataFrame(dd, index, columns=heading[0,0:])\n",
    "df.tail()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Row1 has prob. of being toxic, insulting and obscene\n",
    "\n",
    "Row2 can be considered as being a clean comment\n",
    "\n",
    "Row3 can be considered as being a clean comment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
